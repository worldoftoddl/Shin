#!/usr/bin/env python3
"""
IFRS PDFë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ê°œì„ ëœ ìŠ¤í¬ë¦½íŠ¸
- ë¬¸ì¥ ë‹¨ìœ„ ì²­í¬ ë¶„í•  (NLTK/spaCy)
- êµ¬ì¡°í™”ëœ ë¡œê¹…
- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì¤‘ë³µ ì œê±°
- CLI ì¸ì ì²˜ë¦¬
- ë‹¤ì¤‘ PDF íŒŒì¼ ì²˜ë¦¬ ì§€ì›
"""

import os
import sys
import argparse
import logging
import re
from pathlib import Path
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
from datetime import datetime


@dataclass
class ProcessingConfig:
    """ì²˜ë¦¬ ì„¤ì • ë°ì´í„°í´ë˜ìŠ¤"""
    chunk_size: int = 500
    overlap: int = 100
    min_sentence_length: int = 10
    max_sentence_length: int = 1000
    embedding_model: str = 'jhgan/ko-simcse-roberta'
    collection_name: str = 'ifrs_collection'
    db_path: str = 'chroma_db'


class PDFVectorDBProcessor:
    """PDF ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.logger = self._setup_logging()
        self.sentence_tokenizer = None
        self.model = None
        self.client = None
        
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger('PDFVectorDB')
        logger.setLevel(logging.INFO)
        
        # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬
        log_file = f'pdf_vectordb_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # í¬ë§·í„° ì„¤ì •
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def check_dependencies(self) -> bool:
        """í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        required_modules = {
            'fitz': 'PyMuPDF',
            'sentence_transformers': 'sentence-transformers', 
            'chromadb': 'chromadb',
            'nltk': 'nltk',
            'spacy': 'spacy'
        }
        
        missing = []
        for module, package in required_modules.items():
            try:
                __import__(module)
                self.logger.info(f"âœ“ {package} ì‚¬ìš© ê°€ëŠ¥")
            except ImportError:
                missing.append(package)
                self.logger.error(f"âœ— {package} ì„¤ì¹˜ í•„ìš”")
        
        if missing:
            self.logger.error(f"ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {' '.join(missing)}")
            return False
        return True
    
    def _initialize_nlp_tools(self):
        """NLP ë„êµ¬ ì´ˆê¸°í™”"""
        try:
            import nltk
            from nltk.tokenize import sent_tokenize
            
            # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                self.logger.info("NLTK punkt ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
                nltk.download('punkt')
            
            self.sentence_tokenizer = sent_tokenize
            self.logger.info("NLTK ë¬¸ì¥ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"NLTK ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ë°±ì—…: spaCy ì‚¬ìš©
            try:
                import spacy
                nlp = spacy.load("en_core_web_sm")
                self.sentence_tokenizer = lambda text: [sent.text for sent in nlp(text).sents]
                self.logger.info("spaCy ë¬¸ì¥ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e2:
                self.logger.error(f"spaCy ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e2}")
                # ìµœí›„ ìˆ˜ë‹¨: ì •ê·œì‹ ê¸°ë°˜ ë¶„í• 
                self.sentence_tokenizer = self._regex_sentence_split
                self.logger.warning("ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„í•  ì‚¬ìš©")
    
    def _regex_sentence_split(self, text: str) -> List[str]:
        """ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„í•  (ë°±ì—…ìš©)"""
        sentences = re.split(r'[.!?]+\\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # 1. ê¸°ë³¸ ì •ì œ
        text = re.sub(r'\\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)  # ì œì–´ ë¬¸ì ì œê±°
        text = text.strip()
        
        # 2. íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (í•„ìš”ì‹œ)
        text = re.sub(r'[â€¢â–ªâ–«â—¦â€£âƒ]', '- ', text)  # ë¶ˆë¦¿ í¬ì¸íŠ¸ í†µì¼
        text = re.sub(r'[""''â€šâ€]', '"', text)  # ë”°ì˜´í‘œ í†µì¼
        
        self.logger.debug(f"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(text)} ë¬¸ì")
        return text
    
    def extract_text_from_pdf(self, pdf_path: Union[str, Path]) -> Optional[str]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import fitz
            pdf_path = Path(pdf_path)
            
            if not pdf_path.exists():
                self.logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                return None
            
            self.logger.info(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {pdf_path.name}")
            
            doc = fitz.open(pdf_path)
            text = ""
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += page_text + "\\n"
                
                if page_num % 10 == 0:
                    self.logger.debug(f"í˜ì´ì§€ {page_num + 1}/{len(doc)} ì²˜ë¦¬ ì¤‘...")
            
            doc.close()
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = self.preprocess_text(text)
            
            self.logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ë¬¸ì, {len(doc)} í˜ì´ì§€")
            return text
            
        except Exception as e:
            self.logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def split_text_into_sentence_chunks(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• """
        if not self.sentence_tokenizer:
            self._initialize_nlp_tools()
        
        try:
            # 1. ë¬¸ì¥ ë¶„í• 
            sentences = self.sentence_tokenizer(text)
            self.logger.info(f"ë¬¸ì¥ ë¶„í•  ì™„ë£Œ: {len(sentences)} ë¬¸ì¥")
            
            # 2. ë¬¸ì¥ í•„í„°ë§
            filtered_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if (self.config.min_sentence_length <= len(sentence) <= self.config.max_sentence_length):
                    filtered_sentences.append(sentence)
            
            self.logger.info(f"ë¬¸ì¥ í•„í„°ë§ ì™„ë£Œ: {len(filtered_sentences)} ë¬¸ì¥")
            
            # 3. ì¤‘ë³µ ë¬¸ì¥ ì œê±°
            unique_sentences = list(dict.fromkeys(filtered_sentences))
            removed_count = len(filtered_sentences) - len(unique_sentences)
            if removed_count > 0:
                self.logger.info(f"ì¤‘ë³µ ë¬¸ì¥ {removed_count}ê°œ ì œê±°")
            
            # 4. ì²­í¬ ìƒì„±
            chunks = []
            current_chunk = ""
            
            for sentence in unique_sentences:
                # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
                potential_chunk = current_chunk + " " + sentence if current_chunk else sentence
                
                if len(potential_chunk) <= self.config.chunk_size:
                    current_chunk = potential_chunk
                else:
                    # í˜„ì¬ ì²­í¬ ì €ì¥
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    
                    # ìƒˆ ì²­í¬ ì‹œì‘
                    current_chunk = sentence
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            self.logger.info(f"ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)} ì²­í¬")
            return chunks
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì¥ ì²­í¬ ë¶„í•  ì˜¤ë¥˜: {e}")
            # ë°±ì—…: ê¸°ë³¸ ì²­í¬ ë¶„í• 
            return self._basic_chunk_split(text)
    
    def _basic_chunk_split(self, text: str) -> List[str]:
        """ê¸°ë³¸ ì²­í¬ ë¶„í•  (ë°±ì—…ìš©)"""
        chunks = []
        start = 0
        while start < len(text):
            end = start + self.config.chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - self.config.overlap
        return chunks
    
    def initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from sentence_transformers import SentenceTransformer
            
            self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {self.config.embedding_model}")
            self.model = SentenceTransformer(self.config.embedding_model)
            self.logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            raise
    
    def initialize_chromadb(self, db_path: Path):
        """ChromaDB ì´ˆê¸°í™”"""
        try:
            import chromadb
            
            db_path.mkdir(parents=True, exist_ok=True)
            
            self.client = chromadb.PersistentClient(path=str(db_path))
            self.logger.info(f"ChromaDB ì´ˆê¸°í™” ì™„ë£Œ: {db_path}")
            
        except Exception as e:
            self.logger.error(f"ChromaDB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            raise
    
    def process_pdf_files(self, pdf_paths: List[Path], db_path: Path, rebuild: bool = False):
        """ì—¬ëŸ¬ PDF íŒŒì¼ ì²˜ë¦¬"""
        self.logger.info(f"PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {len(pdf_paths)} íŒŒì¼")
        
        # ì˜ì¡´ì„± í™•ì¸
        if not self.check_dependencies():
            return False
        
        # ëª¨ë¸ ë° DB ì´ˆê¸°í™”
        self.initialize_embedding_model()
        self.initialize_chromadb(db_path)
        
        # ì»¬ë ‰ì…˜ ì„¤ì •
        try:
            if rebuild:
                try:
                    self.client.delete_collection(name=self.config.collection_name)
                    self.logger.info("ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
                except:
                    pass
            
            collection = self.client.create_collection(
                name=self.config.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            
        except Exception as e:
            # ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            collection = self.client.get_collection(name=self.config.collection_name)
            self.logger.info("ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©")
        
        # ê° PDF íŒŒì¼ ì²˜ë¦¬
        total_chunks = 0
        
        for pdf_path in pdf_paths:
            try:
                self.logger.info(f"ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = self.extract_text_from_pdf(pdf_path)
                if not text:
                    continue
                
                # ì²­í¬ ë¶„í• 
                chunks = self.split_text_into_sentence_chunks(text)
                if not chunks:
                    continue
                
                # ì„ë² ë”© ìƒì„±
                self.logger.info("ì„ë² ë”© ìƒì„± ì¤‘...")
                embeddings = self.model.encode(
                    chunks, 
                    convert_to_tensor=True,
                    show_progress_bar=True
                )
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadatas = [
                    {
                        "source": pdf_path.stem,
                        "file_path": str(pdf_path),
                        "chunk_id": i,
                        "total_chunks": len(chunks)
                    } 
                    for i in range(len(chunks))
                ]
                
                # ID ìƒì„±
                ids = [f"{pdf_path.stem}_chunk_{i}" for i in range(len(chunks))]
                
                # ChromaDBì— ì €ì¥
                collection.add(
                    embeddings=embeddings.cpu().numpy().tolist(),
                    documents=chunks,
                    metadatas=metadatas,
                    ids=ids
                )
                
                total_chunks += len(chunks)
                self.logger.info(f"âœ“ {pdf_path.name}: {len(chunks)} ì²­í¬ ì €ì¥ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {pdf_path.name}: {e}")
                continue
        
        self.logger.info(f"ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_chunks} ì²­í¬")
        return True
    
    def query_database(self, query: str, n_results: int = 5) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬"""
        try:
            if not self.client:
                db_path = Path(self.config.db_path) / "ifrs_db"
                self.initialize_chromadb(db_path)
            
            if not self.model:
                self.initialize_embedding_model()
            
            collection = self.client.get_collection(name=self.config.collection_name)
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.model.encode([query], convert_to_tensor=True)
            
            # ê²€ìƒ‰ ì‹¤í–‰
            results = collection.query(
                query_embeddings=query_embedding.cpu().numpy().tolist(),
                n_results=n_results
            )
            
            self.logger.info(f"ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ: '{query}' - {len(results['documents'][0])} ê²°ê³¼")
            return results
            
        except Exception as e:
            self.logger.error(f"ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {}


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='IFRS PDF ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬')
    parser.add_argument('--pdf-dir', type=str, default='.', 
                       help='PDF íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: í˜„ì¬ ë””ë ‰í† ë¦¬)')
    parser.add_argument('--pdf-files', type=str, nargs='+', 
                       help='ì²˜ë¦¬í•  íŠ¹ì • PDF íŒŒì¼ë“¤')
    parser.add_argument('--db-path', type=str, default='chroma_db/ifrs_db',
                       help='ChromaDB ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--rebuild', action='store_true',
                       help='ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶•')
    parser.add_argument('--query', type=str,
                       help='ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    parser.add_argument('--chunk-size', type=int, default=500,
                       help='ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 500)')
    parser.add_argument('--overlap', type=int, default=100,
                       help='ì²­í¬ ì˜¤ë²„ë© (ê¸°ë³¸ê°’: 100)')
    parser.add_argument('--model', type=str, default='jhgan/ko-simcse-roberta',
                       help='ì„ë² ë”© ëª¨ë¸')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = ProcessingConfig(
        chunk_size=args.chunk_size,
        overlap=args.overlap,
        embedding_model=args.model,
        db_path=args.db_path
    )
    
    # í”„ë¡œì„¸ì„œ ìƒì„±
    processor = PDFVectorDBProcessor(config)
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    if args.verbose:
        processor.logger.setLevel(logging.DEBUG)
    
    # ì¿¼ë¦¬ ëª¨ë“œ
    if args.query:
        results = processor.query_database(args.query)
        if results and 'documents' in results:
            print(f"\\nì¿¼ë¦¬: '{args.query}'")
            print("=" * 50)
            for i, doc in enumerate(results['documents'][0]):
                print(f"{i+1}. {doc[:200]}...")
                print("-" * 30)
        return
    
    # PDF íŒŒì¼ ìˆ˜ì§‘
    pdf_paths = []
    
    if args.pdf_files:
        # íŠ¹ì • íŒŒì¼ë“¤
        for pdf_file in args.pdf_files:
            path = Path(pdf_file)
            if path.exists() and path.suffix.lower() == '.pdf':
                pdf_paths.append(path)
    else:
        # ë””ë ‰í† ë¦¬ì—ì„œ PDF íŒŒì¼ ê²€ìƒ‰
        pdf_dir = Path(args.pdf_dir)
        pdf_paths = list(pdf_dir.glob("*.pdf"))
    
    if not pdf_paths:
        print("ì²˜ë¦¬í•  PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬
    db_path = Path(args.db_path)
    success = processor.process_pdf_files(pdf_paths, db_path, args.rebuild)
    
    if success:
        print(f"\\nâœ“ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ: {db_path}")
        print(f"âœ“ ì²˜ë¦¬ëœ íŒŒì¼: {len(pdf_paths)} ê°œ")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        if not args.query:
            test_results = processor.query_database("êµ­ì œíšŒê³„ê¸°ì¤€", n_results=3)
            if test_results:
                print("\\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ê²°ê³¼:")
                for i, doc in enumerate(test_results['documents'][0]):
                    print(f"{i+1}. {doc[:100]}...")
    else:
        print("\\nâœ— ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨")


if __name__ == "__main__":
    main()